{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SimilarityClustering import SimilarityClustering\n",
    "import articles_data\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cnouns as cn\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/somanews')\n",
    "client.somanews.authenticate('ssomanews', 'ssomanews1029')\n",
    "db = client.get_database('somanews')\n",
    "\n",
    "crawled_collection = db.get_collection('crawledArticles')\n",
    "clusters_collection = db.get_collection('bclusters')\n",
    "articles_collection = db.get_collection('barticles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f1f97e355f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_collection.delete_many({})\n",
    "articles_collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1115_00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore_dir = \"../datastore/\"\n",
    "catelist_path = datastore_dir + \"category2.p\"\n",
    "\n",
    "w2v_src_dir = datastore_dir + \"w2v_src4\"\n",
    "w2v_path = datastore_dir + \"sejongcorpus_w2v4_2.p\"\n",
    "\n",
    "nnp_dict_path = datastore_dir + \"nnps2.p\"\n",
    "corpus_path = datastore_dir + \"corpus2.p\"\n",
    "\n",
    "target_time = datetime.datetime(2016, 11, 15)\n",
    "# target_time = datetime.datetime.now()\n",
    "prefix = int(\"%.2d%.2d\"%(target_time.month, target_time.day))\n",
    "prefix_str = \"%d_00\" % prefix\n",
    "prefix_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnp_dict_df = pd.read_pickle(nnp_dict_path)\n",
    "nnp_dict_df = nnp_dict_df[nnp_dict_df>10]\n",
    "nnp_dict = nnp_dict_df.index.tolist()\n",
    "\n",
    "custom_dict = [u'새누리', u'새누리당', u'더민주', u'더민주당', u'최순실', u'박대통령', u'국회의장', u'야권의요구', u'정기국회', u'참여정부']\n",
    "dicts = set(nnp_dict + custom_dict)\n",
    "\n",
    "def tokenizer(inp_str):\n",
    "#     return cn.pos_tags(inp_str)\n",
    "   return cn.custom_pos_tags(inp_str, dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4656"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nnp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles - 152728\n",
      "Number of corpus - 1273272\n",
      "Number of words - 1426000\n",
      "Number of batchs - 286\n",
      "Batch#0 - tokenizer took 33.838441 sec\n",
      "Batch#1 - tokenizer took 2.497328 sec\n",
      "Batch#2 - tokenizer took 2.663000 sec\n",
      "Batch#3 - tokenizer took 2.139003 sec\n",
      "Batch#4 - tokenizer took 2.217016 sec\n",
      "Batch#5 - tokenizer took 2.371563 sec\n",
      "Batch#6 - tokenizer took 2.374850 sec\n",
      "Batch#7 - tokenizer took 3.257765 sec\n",
      "Batch#8 - tokenizer took 2.955734 sec\n",
      "Batch#9 - tokenizer took 2.542470 sec\n",
      "Batch#10 - tokenizer took 2.188086 sec\n",
      "Batch#11 - tokenizer took 2.948571 sec\n",
      "Batch#12 - tokenizer took 2.573278 sec\n",
      "Batch#13 - tokenizer took 3.022088 sec\n",
      "Batch#14 - tokenizer took 3.126356 sec\n",
      "Batch#15 - tokenizer took 2.610606 sec\n",
      "Batch#16 - tokenizer took 2.654840 sec\n",
      "Batch#17 - tokenizer took 2.354962 sec\n",
      "Batch#18 - tokenizer took 4.096016 sec\n",
      "Batch#19 - tokenizer took 2.844310 sec\n",
      "Batch#20 - tokenizer took 2.833062 sec\n",
      "Batch#21 - tokenizer took 3.312590 sec\n",
      "Batch#22 - tokenizer took 2.290550 sec\n",
      "Batch#23 - tokenizer took 2.764246 sec\n",
      "Batch#24 - tokenizer took 1.902081 sec\n",
      "Batch#25 - tokenizer took 2.790054 sec\n",
      "Batch#26 - tokenizer took 2.812588 sec\n",
      "Batch#27 - tokenizer took 1.666413 sec\n",
      "Batch#28 - tokenizer took 4.062212 sec\n",
      "Batch#29 - tokenizer took 2.498129 sec\n",
      "Batch#30 - tokenizer took 3.567786 sec\n",
      "Batch#31 - tokenizer took 3.730757 sec\n",
      "Batch#32 - tokenizer took 2.429335 sec\n",
      "Batch#33 - tokenizer took 2.379784 sec\n",
      "Batch#34 - tokenizer took 3.047545 sec\n",
      "Batch#35 - tokenizer took 2.527660 sec\n",
      "Batch#36 - tokenizer took 3.668239 sec\n",
      "Batch#37 - tokenizer took 2.988331 sec\n",
      "Batch#38 - tokenizer took 2.069945 sec\n",
      "Batch#39 - tokenizer took 2.418416 sec\n",
      "Batch#40 - tokenizer took 4.367864 sec\n",
      "Batch#41 - tokenizer took 2.749528 sec\n",
      "Batch#42 - tokenizer took 3.551286 sec\n",
      "Batch#43 - tokenizer took 3.184203 sec\n",
      "Batch#44 - tokenizer took 2.644009 sec\n",
      "Batch#45 - tokenizer took 4.013700 sec\n",
      "Batch#46 - tokenizer took 2.917589 sec\n",
      "Batch#47 - tokenizer took 1.948837 sec\n",
      "Batch#48 - tokenizer took 2.704181 sec\n",
      "Batch#49 - tokenizer took 3.536172 sec\n",
      "Batch#50 - tokenizer took 2.439479 sec\n",
      "Batch#51 - tokenizer took 2.944254 sec\n",
      "Batch#52 - tokenizer took 1.900061 sec\n",
      "Batch#53 - tokenizer took 2.692346 sec\n",
      "Batch#54 - tokenizer took 2.492149 sec\n",
      "Batch#55 - tokenizer took 4.997656 sec\n",
      "Batch#56 - tokenizer took 3.462773 sec\n",
      "Batch#57 - tokenizer took 2.888372 sec\n",
      "Batch#58 - tokenizer took 3.686272 sec\n",
      "Batch#59 - tokenizer took 2.945730 sec\n",
      "Batch#60 - tokenizer took 4.126692 sec\n",
      "Batch#61 - tokenizer took 3.212583 sec\n",
      "Batch#62 - tokenizer took 3.991627 sec\n",
      "Batch#63 - tokenizer took 4.333347 sec\n",
      "Batch#64 - tokenizer took 3.645442 sec\n",
      "Batch#65 - tokenizer took 5.215763 sec\n",
      "Batch#66 - tokenizer took 2.570188 sec\n",
      "Batch#67 - tokenizer took 3.378965 sec\n",
      "Batch#68 - tokenizer took 2.173622 sec\n",
      "Batch#69 - tokenizer took 2.599220 sec\n",
      "Batch#70 - tokenizer took 2.670442 sec\n",
      "Batch#71 - tokenizer took 2.415132 sec\n",
      "Batch#72 - tokenizer took 3.073101 sec\n",
      "Batch#73 - tokenizer took 2.353110 sec\n",
      "Batch#74 - tokenizer took 2.726474 sec\n",
      "Batch#75 - tokenizer took 2.858604 sec\n",
      "Batch#76 - tokenizer took 3.959050 sec\n",
      "Batch#77 - tokenizer took 2.799556 sec\n",
      "Batch#78 - tokenizer took 2.463329 sec\n",
      "Batch#79 - tokenizer took 4.064197 sec\n",
      "Batch#80 - tokenizer took 2.387912 sec\n",
      "Batch#81 - tokenizer took 2.603074 sec\n",
      "Batch#82 - tokenizer took 3.737724 sec\n",
      "Batch#83 - tokenizer took 4.018170 sec\n",
      "Batch#84 - tokenizer took 2.661053 sec\n",
      "Batch#85 - tokenizer took 2.793325 sec\n",
      "Batch#86 - tokenizer took 3.602346 sec\n",
      "Batch#87 - tokenizer took 2.873550 sec\n",
      "Batch#88 - tokenizer took 2.265141 sec\n",
      "Batch#89 - tokenizer took 2.744724 sec\n",
      "Batch#90 - tokenizer took 2.390441 sec\n",
      "Batch#91 - tokenizer took 1.998863 sec\n",
      "Batch#92 - tokenizer took 3.113141 sec\n",
      "Batch#93 - tokenizer took 3.782501 sec\n",
      "Batch#94 - tokenizer took 3.477361 sec\n",
      "Batch#95 - tokenizer took 3.225807 sec\n",
      "Batch#96 - tokenizer took 4.056093 sec\n",
      "Batch#97 - tokenizer took 3.323545 sec\n",
      "Batch#98 - tokenizer took 2.472635 sec\n",
      "Batch#99 - tokenizer took 3.621730 sec\n",
      "Batch#100 - tokenizer took 3.071873 sec\n",
      "Batch#101 - tokenizer took 3.016508 sec\n",
      "Batch#102 - tokenizer took 3.423721 sec\n",
      "Batch#103 - tokenizer took 2.248429 sec\n",
      "Batch#104 - tokenizer took 2.669159 sec\n",
      "Batch#105 - tokenizer took 1.602781 sec\n",
      "Batch#106 - tokenizer took 2.136096 sec\n",
      "Batch#107 - tokenizer took 3.210365 sec\n",
      "Batch#108 - tokenizer took 3.506286 sec\n",
      "Batch#109 - tokenizer took 3.843527 sec\n",
      "Batch#110 - tokenizer took 3.799587 sec\n",
      "Batch#111 - tokenizer took 3.213535 sec\n",
      "Batch#112 - tokenizer took 3.223342 sec\n",
      "Batch#113 - tokenizer took 3.115061 sec\n",
      "Batch#114 - tokenizer took 2.271500 sec\n",
      "Batch#115 - tokenizer took 2.477384 sec\n",
      "Batch#116 - tokenizer took 2.456558 sec\n",
      "Batch#117 - tokenizer took 3.912002 sec\n",
      "Batch#118 - tokenizer took 3.702979 sec\n",
      "Batch#119 - tokenizer took 3.072640 sec\n",
      "Batch#120 - tokenizer took 3.170027 sec\n",
      "Batch#121 - tokenizer took 3.818765 sec\n",
      "Batch#122 - tokenizer took 2.590988 sec\n",
      "Batch#123 - tokenizer took 2.022611 sec\n",
      "Batch#124 - tokenizer took 3.024461 sec\n",
      "Batch#125 - tokenizer took 2.528387 sec\n",
      "Batch#126 - tokenizer took 3.051468 sec\n",
      "Batch#127 - tokenizer took 2.444582 sec\n",
      "Batch#128 - tokenizer took 4.059378 sec\n",
      "Batch#129 - tokenizer took 3.635164 sec\n",
      "Batch#130 - tokenizer took 3.032644 sec\n",
      "Batch#131 - tokenizer took 1.804598 sec\n",
      "Batch#132 - tokenizer took 3.147793 sec\n",
      "Batch#133 - tokenizer took 3.076239 sec\n",
      "Batch#134 - tokenizer took 3.519531 sec\n",
      "Batch#135 - tokenizer took 3.450266 sec\n",
      "Batch#136 - tokenizer took 3.294452 sec\n",
      "Batch#137 - tokenizer took 3.233505 sec\n",
      "Batch#138 - tokenizer took 3.246887 sec\n",
      "Batch#139 - tokenizer took 4.447092 sec\n",
      "Batch#140 - tokenizer took 3.739763 sec\n",
      "Batch#141 - tokenizer took 3.565763 sec\n",
      "Batch#142 - tokenizer took 3.307263 sec\n",
      "Batch#143 - tokenizer took 5.739717 sec\n",
      "Batch#144 - tokenizer took 3.422948 sec\n",
      "Batch#145 - tokenizer took 1.857009 sec\n",
      "Batch#146 - tokenizer took 2.497710 sec\n",
      "Batch#147 - tokenizer took 2.324627 sec\n",
      "Batch#148 - tokenizer took 2.915146 sec\n",
      "Batch#149 - tokenizer took 2.063323 sec\n",
      "Batch#150 - tokenizer took 2.549215 sec\n",
      "Batch#151 - tokenizer took 2.372737 sec\n",
      "Batch#152 - tokenizer took 2.947137 sec\n",
      "Batch#153 - tokenizer took 2.670108 sec\n",
      "Batch#154 - tokenizer took 2.541568 sec\n",
      "Batch#155 - tokenizer took 4.002411 sec\n",
      "Batch#156 - tokenizer took 2.605288 sec\n",
      "Batch#157 - tokenizer took 3.155466 sec\n",
      "Batch#158 - tokenizer took 2.930984 sec\n",
      "Batch#159 - tokenizer took 2.587036 sec\n",
      "Batch#160 - tokenizer took 2.467157 sec\n",
      "Batch#161 - tokenizer took 2.515102 sec\n",
      "Batch#162 - tokenizer took 2.432507 sec\n",
      "Batch#163 - tokenizer took 2.340198 sec\n",
      "Batch#164 - tokenizer took 3.569949 sec\n",
      "Batch#165 - tokenizer took 2.286644 sec\n",
      "Batch#166 - tokenizer took 2.238621 sec\n",
      "Batch#167 - tokenizer took 1.884293 sec\n",
      "Batch#168 - tokenizer took 2.986560 sec\n",
      "Batch#169 - tokenizer took 3.012209 sec\n",
      "Batch#170 - tokenizer took 2.889151 sec\n",
      "Batch#171 - tokenizer took 2.675253 sec\n",
      "Batch#172 - tokenizer took 2.850038 sec\n",
      "Batch#173 - tokenizer took 2.810914 sec\n",
      "Batch#174 - tokenizer took 2.774772 sec\n",
      "Batch#175 - tokenizer took 2.616106 sec\n",
      "Batch#176 - tokenizer took 2.846062 sec\n",
      "Batch#177 - tokenizer took 2.535839 sec\n",
      "Batch#178 - tokenizer took 3.700686 sec\n",
      "Batch#179 - tokenizer took 2.367527 sec\n",
      "Batch#180 - tokenizer took 2.356927 sec\n",
      "Batch#181 - tokenizer took 3.540836 sec\n",
      "Batch#182 - tokenizer took 2.618315 sec\n",
      "Batch#183 - tokenizer took 2.804604 sec\n",
      "Batch#184 - tokenizer took 2.879356 sec\n",
      "Batch#185 - tokenizer took 2.501935 sec\n",
      "Batch#186 - tokenizer took 1.834791 sec\n",
      "Batch#187 - tokenizer took 4.304319 sec\n",
      "Batch#188 - tokenizer took 3.131666 sec\n",
      "Batch#189 - tokenizer took 3.141100 sec\n",
      "Batch#190 - tokenizer took 1.896890 sec\n",
      "Batch#191 - tokenizer took 2.196472 sec\n",
      "Batch#192 - tokenizer took 3.158887 sec\n",
      "Batch#193 - tokenizer took 2.700707 sec\n",
      "Batch#194 - tokenizer took 3.280732 sec\n",
      "Batch#195 - tokenizer took 3.658862 sec\n",
      "Batch#196 - tokenizer took 3.486257 sec\n",
      "Batch#197 - tokenizer took 3.223517 sec\n",
      "Batch#198 - tokenizer took 2.360166 sec\n",
      "Batch#199 - tokenizer took 3.874642 sec\n",
      "Batch#200 - tokenizer took 3.170695 sec\n",
      "Batch#201 - tokenizer took 3.199028 sec\n",
      "Batch#202 - tokenizer took 3.332453 sec\n",
      "Batch#203 - tokenizer took 3.010842 sec\n",
      "Batch#204 - tokenizer took 3.038851 sec\n",
      "Batch#205 - tokenizer took 2.743426 sec\n",
      "Batch#206 - tokenizer took 2.792570 sec\n",
      "Batch#207 - tokenizer took 2.619238 sec\n",
      "Batch#208 - tokenizer took 1.690230 sec\n",
      "Batch#209 - tokenizer took 187.886128 sec\n",
      "Batch#210 - tokenizer took 3.166408 sec\n",
      "Batch#211 - tokenizer took 2.703351 sec\n",
      "Batch#212 - tokenizer took 2.839363 sec\n",
      "Batch#213 - tokenizer took 3.254660 sec\n",
      "Batch#214 - tokenizer took 3.287695 sec\n",
      "Batch#215 - tokenizer took 2.990927 sec\n",
      "Batch#216 - tokenizer took 2.751619 sec\n",
      "Batch#217 - tokenizer took 2.587170 sec\n",
      "Batch#218 - tokenizer took 2.069017 sec\n",
      "Batch#219 - tokenizer took 1.737732 sec\n",
      "Batch#220 - tokenizer took 2.137142 sec\n",
      "Batch#221 - tokenizer took 1.994773 sec\n",
      "Batch#222 - tokenizer took 2.819169 sec\n",
      "Batch#223 - tokenizer took 1.989571 sec\n",
      "Batch#224 - tokenizer took 3.512295 sec\n",
      "Batch#225 - tokenizer took 2.609446 sec\n",
      "Batch#226 - tokenizer took 3.341113 sec\n",
      "Batch#227 - tokenizer took 2.465091 sec\n",
      "Batch#228 - tokenizer took 2.872617 sec\n",
      "Batch#229 - tokenizer took 1.833491 sec\n",
      "Batch#230 - tokenizer took 2.066308 sec\n",
      "Batch#231 - tokenizer took 3.091471 sec\n",
      "Batch#232 - tokenizer took 2.413380 sec\n",
      "Batch#233 - tokenizer took 2.537203 sec\n",
      "Batch#234 - tokenizer took 2.560711 sec\n",
      "Batch#235 - tokenizer took 3.989130 sec\n",
      "Batch#236 - tokenizer took 3.520024 sec\n",
      "Batch#237 - tokenizer took 4.632746 sec\n",
      "Batch#238 - tokenizer took 2.209519 sec\n",
      "Batch#239 - tokenizer took 3.457293 sec\n",
      "Batch#240 - tokenizer took 2.564332 sec\n",
      "Batch#241 - tokenizer took 2.969226 sec\n",
      "Batch#242 - tokenizer took 2.784272 sec\n",
      "Batch#243 - tokenizer took 2.618154 sec\n",
      "Batch#244 - tokenizer took 2.559036 sec\n",
      "Batch#245 - tokenizer took 2.535051 sec\n",
      "Batch#246 - tokenizer took 2.466280 sec\n",
      "Batch#247 - tokenizer took 2.854888 sec\n",
      "Batch#248 - tokenizer took 2.843610 sec\n",
      "Batch#249 - tokenizer took 2.976236 sec\n",
      "Batch#250 - tokenizer took 3.262376 sec\n",
      "Batch#251 - tokenizer took 3.125065 sec\n",
      "Batch#252 - tokenizer took 2.396887 sec\n",
      "Batch#253 - tokenizer took 2.884249 sec\n",
      "Batch#254 - tokenizer took 9.774271 sec\n",
      "Batch#255 - tokenizer took 19.250916 sec\n",
      "Batch#256 - tokenizer took 17.092207 sec\n",
      "Batch#257 - tokenizer took 18.609014 sec\n",
      "Batch#258 - tokenizer took 17.512099 sec\n",
      "Batch#259 - tokenizer took 17.143055 sec\n",
      "Batch#260 - tokenizer took 19.188874 sec\n",
      "Batch#261 - tokenizer took 16.858186 sec\n",
      "Batch#262 - tokenizer took 19.684355 sec\n",
      "Batch#263 - tokenizer took 16.698230 sec\n",
      "Batch#264 - tokenizer took 16.860683 sec\n",
      "Batch#265 - tokenizer took 19.763261 sec\n",
      "Batch#266 - tokenizer took 18.335761 sec\n",
      "Batch#267 - tokenizer took 16.247538 sec\n",
      "Batch#268 - tokenizer took 15.803683 sec\n",
      "Batch#269 - tokenizer took 23.453530 sec\n",
      "Batch#270 - tokenizer took 17.364203 sec\n",
      "Batch#271 - tokenizer took 17.500160 sec\n",
      "Batch#272 - tokenizer took 17.448035 sec\n",
      "Batch#273 - tokenizer took 16.435316 sec\n",
      "Batch#274 - tokenizer took 24.462791 sec\n",
      "Batch#275 - tokenizer took 17.677974 sec\n",
      "Batch#276 - tokenizer took 17.746477 sec\n",
      "Batch#277 - tokenizer took 18.712509 sec\n",
      "Batch#278 - tokenizer took 18.817524 sec\n",
      "Batch#279 - tokenizer took 18.108304 sec\n",
      "Batch#280 - tokenizer took 17.538958 sec\n",
      "Batch#281 - tokenizer took 26.978526 sec\n",
      "Batch#282 - tokenizer took 16.765028 sec\n",
      "Batch#283 - tokenizer took 19.321507 sec\n",
      "Batch#284 - tokenizer took 19.279641 sec\n",
      "Batch#285 - tokenizer took 4.010295 sec\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "size = articles_data.makeDataset(crawled_collection, w2v_src_dir, corpus_path, tokenizer=tokenizer)\n",
    "print size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2287d1bc1a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_src_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "articles_data.trainWord2Vec(w2v_src_dir, w2v_path, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = articles_data.find_recent_articles(crawled_collection, catelist_path, target_time)\n",
    "sc = SimilarityClustering()\n",
    "sc.train(\"cate\", w2v_path, train_df, path=\"../datastore\", prefix=prefix_str, tokenizer=tokenizer,\n",
    "            threshold=0.8, \n",
    "            cnt_threshold=2, \n",
    "            repeat=1,\n",
    "            model_name='dbow+dmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SimilarityClustering.load(only_d2v=False, path=\"../datastore\", prefix=prefix_str, model_name='dbow+dmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.select_model('dbow+dmm')\n",
    "sc.cluster_train(\"cate\", path=\"../datastore\", prefix=prefix_str, repeat=1, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.save(path=\"../datastore\", prefix=prefix_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calced_clusters = sc.save_to_db(prefix, clusters_collection, articles_collection, target_time, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calced_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.iner_score(threshold=0.7, cnt_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.print_clusters(top=15, sortby='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(calced_clusters, columns=['cluster', 'portion', 'deltaTime', 'cohesion', 'ntc'])\n",
    "sort_cdf = cdf.sort_values('ntc', ascending=False)\n",
    "sort_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, row in sort_cdf.iterrows():\n",
    "    for c in calced_clusters:\n",
    "        if(c['cluster'] == row.cluster):\n",
    "            print c['ntc'], c['leading']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
