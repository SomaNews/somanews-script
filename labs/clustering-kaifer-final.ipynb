{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "import math\n",
    "import hanja\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import cnouns as cn\n",
    "import check_utils as cu\n",
    "import deep_utils as du\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim import models\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "import cPickle as pickle\n",
    "from spherecluster import SphericalKMeans\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Articles = namedtuple('Articles', 'words tags split')\n",
    "\n",
    "class SimilarityClustering:\n",
    "    # df_\n",
    "    \n",
    "    # times_\n",
    "    \n",
    "    # alldocs_\n",
    "    # models_by_name_\n",
    "    # dm_\n",
    "    # centers_\n",
    "    # scores_\n",
    "    # countby_\n",
    "    # topics_\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.times_ = {\n",
    "            \"preprocessing\": {},\n",
    "            \"learning\": {},\n",
    "            \"clustering\": {},\n",
    "            \"topic\": {}\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def reset(self, train_df):\n",
    "        self.df_ = train_df[:]\n",
    "        \n",
    "        \n",
    "    def tokenize(self, tokenizer=cn.tokenize):\n",
    "        self.times_[\"preprocessing\"][\"start\"] = time()\n",
    "        self.alldocs_ = []\n",
    "        size = len(self.df_) / 4\n",
    "        \n",
    "        ## tokenize\n",
    "        print(\"Tokenizing.....\")\n",
    "        self.df_['target_str'] = [tokenizer(row.title + \" \" + row.content) for idx, row in self.df_.iterrows()]\n",
    "        print(\"Complete to tokenize.\")\n",
    "        \n",
    "        ## make docs\n",
    "        for idx, row in self.df_.iterrows():\n",
    "            tokens = row['target_str'].split(' ')\n",
    "            words = tokens[0:]\n",
    "            tags = [idx]\n",
    "            tmp = idx//size % 4\n",
    "            split = ['train','test','extra','extra'][tmp]  # 25k train, 25k test, 25k extra\n",
    "            self.alldocs_.append(Articles(words, tags, split))\n",
    "            \n",
    "        self.times_[\"preprocessing\"][\"end\"] = time()\n",
    "    \n",
    "    \n",
    "    def doc_train(self, w2v_path=\"../datastore/sejongcorpus_w2v2.p\", alpha=0.025, min_alpha=0.001, passes=20):\n",
    "        self.times_[\"learning\"][\"start\"] = time()\n",
    "        \n",
    "        ## make doc2vec models\n",
    "        simple_models = [\n",
    "            # PV-DM Distributed Momory Model of PV\n",
    "            # w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "            Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "            # PV-DBOW Distributed Bag of Words version of PV\n",
    "            Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "            # PV-DM w/average\n",
    "            Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        ]\n",
    "        \n",
    "        ## load word2vec model\n",
    "        print(\"Loading word2vec model.....\")\n",
    "        simple_models[0].load_word2vec_format(w2v_path)\n",
    "        print(\"Complete to load.\")\n",
    "        \n",
    "        ## reset doc2vec model\n",
    "        simple_models[0].build_vocab(self.alldocs_)\n",
    "        print simple_models[0]\n",
    "        for model in simple_models[1:]:\n",
    "            model.reset_from(simple_models[0])\n",
    "            print(model)\n",
    "\n",
    "        self.models_by_name_ = OrderedDict((str(model), model) for model in simple_models)\n",
    "        \n",
    "        ## training\n",
    "        doc_list = self.alldocs_[:]\n",
    "        alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "        for epoch in range(passes):\n",
    "            shuffle(doc_list)  # shuffling gets best results\n",
    "\n",
    "            for name, train_model in self.models_by_name_.items():\n",
    "                train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "                train_model.train(doc_list)\n",
    "                print(\"%i passes : %s\" % (epoch + 1, name))\n",
    "\n",
    "            print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "            alpha -= alpha_delta\n",
    "    \n",
    "        self.concat_vec()\n",
    "        self.times_[\"learning\"][\"end\"] = time()\n",
    "        \n",
    "    \n",
    "    def select_model(self, model_name='Doc2Vec(dm/c,d100,n5,w5,mc2,t8)'):\n",
    "        self.dm_ = self.models_by_name_[model_name]\n",
    "        \n",
    "        \n",
    "    def clustering(self, threshold=0.8):\n",
    "        self.times_[\"clustering\"][\"start\"] = time()\n",
    "        print(\"Similarity clustering.....\")\n",
    "        self.centers_ = du.similarity_clustering(self.df_, self.dm_.docvecs, threshold)\n",
    "        du.calc_similarity(self.df_, self.dm_.docvecs, self.centers_)\n",
    "        print(\"Complete to similarity clustering.\")\n",
    "        self.times_[\"clustering\"][\"end\"] = time()\n",
    "        \n",
    "        \n",
    "    def iner_score(self, cnt_threshold=10):\n",
    "        self.scores_ = du.similarity_iner_score(self.centers_, self.df_, self.dm_.docvecs)\n",
    "        size_1 = self.scores_[self.scores_.cnt==1]\n",
    "        self.countby_ = self.scores_[self.scores_.cnt>cnt_threshold]\n",
    "        print \"total:\", len(self.scores_), \", size_1:\",len(size_1), \", countby:\", len(self.countby_)\n",
    "        ss = self.countby_.sum(axis=0)\n",
    "        print \"distance:\", ss['distance'] * 100\n",
    "        print \"variance:\", ss['variance']\n",
    "        print \"similarity:\", (ss['similarity'] * 100)/len(self.countby_)\n",
    "        print \"cohesion:\", ss['cohesion']\n",
    "        \n",
    "        \n",
    "    def get_all_topics(self, get_topic_func=du.get_all_topics):\n",
    "        self.times_[\"topic\"][\"start\"] = time()\n",
    "        print(\"Get topics.....\")\n",
    "        self.topics_ = get_topic_func(self.df_, self.countby_.cluster.tolist())\n",
    "        print(\"Complete to get topics.\")\n",
    "        self.times_[\"topic\"][\"end\"] = time()\n",
    "        \n",
    "        \n",
    "    def calc_elapsed(self):\n",
    "        for key, value in self.times_.iteritems():\n",
    "            value[\"elapsed\"]= value[\"end\"] - value[\"start\"]\n",
    "            \n",
    "            \n",
    "    def save(self, path, prefix):\n",
    "        self.models_by_name_['Doc2Vec(dm/c,d100,n5,w5,mc2,t8)'].save(\"%s/%sd2v-dmc.p\" % (path, prefix))\n",
    "        self.models_by_name_['Doc2Vec(dbow,d100,n5,mc2,t8)'].save(\"%s/%sd2v-dbow.p\" % (path, prefix))\n",
    "        self.models_by_name_['Doc2Vec(dm/m,d100,n5,w10,mc2,t8)'].save(\"%s/%sd2v-dmm.p\" % (path, prefix))\n",
    "\n",
    "        self.df_.to_pickle(\"%s/%sdf.p\" % (path, prefix))\n",
    "        \n",
    "        pickle.dump(self.centers_, open(\"%s/%scenters.p\" % (path, prefix), \"wb\"))\n",
    "        pickle.dump(self.topics_, open(\"%s/%stopics.p\" % (path, prefix), \"wb\"))\n",
    "        pickle.dump(self.times_, open(\"%s/%stimes.p\" % (path, prefix), \"wb\"))\n",
    "\n",
    "        print(\"Complete to save model.\")\n",
    "        \n",
    "    def concat_vec(self):\n",
    "        self.models_by_name_['dbow+dmm'] = ConcatenatedDoc2Vec([self.models_by_name_['Doc2Vec(dbow,d100,n5,mc2,t8)'], self.models_by_name_['Doc2Vec(dm/m,d100,n5,w10,mc2,t8)']])\n",
    "        self.models_by_name_['dbow+dmc'] = ConcatenatedDoc2Vec([self.models_by_name_['Doc2Vec(dbow,d100,n5,mc2,t8)'], self.models_by_name_['Doc2Vec(dm/c,d100,n5,w5,mc2,t8)']])\n",
    "\n",
    "        \n",
    "    def s_load(path, prefix):\n",
    "        sc = SimilarityClustering()\n",
    "        \n",
    "        sc.models_by_name_ = OrderedDict()\n",
    "        \n",
    "        sc.models_by_name_['Doc2Vec(dm/c,d100,n5,w5,mc2,t8)'] = Doc2Vec.load(\"%s/%sd2v-dmc.p\" % (path, prefix))\n",
    "        sc.models_by_name_['Doc2Vec(dbow,d100,n5,mc2,t8)'] = Doc2Vec.load(\"%s/%sd2v-dbow.p\" % (path, prefix))\n",
    "        sc.models_by_name_['Doc2Vec(dm/m,d100,n5,w10,mc2,t8)'] = Doc2Vec.load(\"%s/%sd2v-dmm.p\" % (path, prefix))\n",
    "\n",
    "        sc.concat_vec()\n",
    "        sc.select_model()\n",
    "        \n",
    "        sc.df_ = pd.read_pickle(\"%s/%sdf.p\" % (path, prefix))\n",
    "        \n",
    "        sc.centers_ = pickle.load(open(\"%s/%scenters.p\" % (path, prefix), \"rb\"))\n",
    "        sc.topics_ = pickle.load(open(\"%s/%stopics.p\" % (path, prefix), \"rb\"))\n",
    "        sc.times_ = pickle.load(open(\"%s/%stimes.p\" % (path, prefix), \"rb\"))\n",
    "\n",
    "        sc.iner_score()\n",
    "        \n",
    "        return sc\n",
    "    load=staticmethod(s_load)\n",
    "    \n",
    "    def print_clusters(size, top=10):\n",
    "        for idx, row in self.countby_.sort_values('cohesion', ascending=False)[:top].iterrows():\n",
    "            print du.test_print(row.cluster, self.df_, self.dm_.docvecs, self.centers_, self.topics_, self.countby_)\n",
    "            print \"\\n------------------------------------------------------------\\n\"\n",
    "            \n",
    "            \n",
    "    def train(self, train_df, path, prefix,\n",
    "              tokenizer=cn.tokenize, \n",
    "              w2v_path=\"../datastore/sejongcorpus_w2v2.p\", alpha=0.025, min_alpha=0.001, passes=20,\n",
    "              model_name='Doc2Vec(dm/c,d100,n5,w5,mc2,t8)', \n",
    "              threshold=0.8, \n",
    "              cnt_threshold=10, \n",
    "              get_topic_func=du.get_all_topics\n",
    "             ):\n",
    "        self.reset(train_df[:])\n",
    "        self.tokenize(cn.tokenize)\n",
    "        self.doc_train(w2v_path, alpha, min_alpha, passes)\n",
    "        self.select_model(model_name)\n",
    "        self.clustering(threshold)\n",
    "        self.iner_score(cnt_threshold)\n",
    "        self.get_all_topics(get_topic_func)\n",
    "        self.calc_elapsed()\n",
    "        self.save(path, prefix)\n",
    "            \n",
    "            \n",
    "#     def save_to_db(db):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model.....\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid vector on line 2 (is this really the text format?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2f22827ce16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_recent_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimilarityClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../datastore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1103_00_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-cfd2d6b1ac51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_df, path, prefix, tokenizer, w2v_path, alpha, min_alpha, passes, model_name, threshold, cnt_threshold, get_topic_func)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cfd2d6b1ac51>\u001b[0m in \u001b[0;36mdoc_train\u001b[0;34m(self, w2v_path, alpha, min_alpha, passes)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m## load word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading word2vec model.....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load word2vec model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whyask37/.pyenv/versions/py27/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid vector on line 2 (is this really the text format?)"
     ]
    }
   ],
   "source": [
    "train_df = articles_data.find_recent_articles()\n",
    "sc = SimilarityClustering()\n",
    "sc.train(train_df, path=\"../datastore\", prefix=\"1103_00_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SimilarityClustering.load(path=\"../datastore\", prefix=\"1103_00_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cnouns as cn\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/somanews')\n",
    "db = client.get_database('somanews')\n",
    "articles = db.get_collection('articles')\n",
    "articles2 = db.get_collection('articles2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144028"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141573"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles2.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
